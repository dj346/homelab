
alloy:
  clustering:
    enabled: false
    name: ""
    portName: http

  configMap:
    create: true
    content: |-
      prometheus.remote_write "mimir" {
        endpoint {
          url     = "http://mimir-distributor.mimir.svc.cluster.local:8080/api/v1/push"
          
          headers = {
            "X-Scope-OrgID" = "anonymous",
          }
        }
      }

      loki.write "loki" {
        endpoint {
          url     = "http://loki-distributor.loki.svc.cluster.local:3100/api/v1/push"
          
          tenant_id = "anonymous"
        }
      }

      prometheus.exporter.self "self_metrics" {}

      prometheus.scrape "self_scrape" {
        targets    = prometheus.exporter.self.self_metrics.targets
        forward_to = [prometheus.remote_write.mimir.receiver]
      }

      prometheus.operator.servicemonitors "mimir_smon" {
        forward_to = [prometheus.remote_write.mimir.receiver]

        namespaces = ["mimir"]
      }

      prometheus.operator.servicemonitors "grafana_smon" {
        forward_to = [prometheus.remote_write.mimir.receiver]

        namespaces = ["grafana"]
      }

      mimir.rules.kubernetes "mimir_rules" {
        address = "http://mimir-ruler.mimir.svc.cluster.local:8080"

        tenant_id = "anonymous"

        rule_selector {
          match_labels = {
            "app.kubernetes.io/instance" = "mimir",
          }
        }
      }

      local.file_match "hostlogs" {
        path_targets = [
          { __path__ = "/host/var/log/**/*.log" }
        ]
      }

      loki.source.file "hostlogs" {
        targets    = local.file_match.hostlogs.targets
        forward_to = [loki.write.loki.receiver]
        tail_from_end = true
      }

  storagePath: /tmp/alloy

  listenAddr: 0.0.0.0
  listenPort: 12345
  listenScheme: HTTP

  # Telemetry configuration
  enableReporting: true

  mounts:
    varlog: true
    dockercontainers: true
    extra: []

  resources: {}

configReloader:
  enabled: true

  resources:
    requests:
      cpu: "10m"
      memory: "50Mi"

controller:
  replicas: 1

  extraAnnotations: {}
  podAnnotations: {}
  podLabels: {}

  

  # -- PodDisruptionBudget configuration.
  podDisruptionBudget:
    # -- Whether to create a PodDisruptionBudget for the controller.
    enabled: false
    # -- Minimum number of pods that must be available during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    minAvailable: null
    # -- Maximum number of pods that can be unavailable during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    maxUnavailable: null

  # -- Whether to enable automatic deletion of stale PVCs due to a scale down operation, when controller.type is 'statefulset'.
  enableStatefulSetAutoDeletePVC: false

  autoscaling:
    horizontal:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80

      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300

      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
    vertical:
      enabled: false
      recommenders: []
      resourcePolicy:
        containerPolicies:
        - containerName: alloy
          controlledResources:
          - cpu
          - memory
          controlledValues: "RequestsAndLimits"
          maxAllowed: {}
          minAllowed: {}

service:
  enabled: true
  type: ClusterIP
  nodePort: 31128
  clusterIP: ''
  internalTrafficPolicy: Cluster
  annotations: {}

serviceMonitor:
  enabled: false
  additionalLabels: {}
  interval: ""
  metricRelabelings: []
  tlsConfig: {}
  relabelings: []

ingress:
  enabled: false
  annotations: {}
  labels: {}
  path: /
  faroPort: 12347
  pathType: Prefix
  hosts:
    - chart-example.local
  extraPaths: []

  tls: []