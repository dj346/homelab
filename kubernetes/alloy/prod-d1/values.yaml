
alloy:
  extraEnv:
    - name: KUBE_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName

  extraPorts:
    # Zipkin (only if you still need it)
    - name: zipkin
      port:       9411
      targetPort: 9411
      protocol:   TCP

    # OTLP gRPC receiver
    - name: otlp-grpc
      port:       4317
      targetPort: 4317
      protocol:   TCP

    # OTLP HTTP/protobuf receiver
    - name: otlp-http
      port:       4318
      targetPort: 4318
      protocol:   TCP

  clustering:
    enabled: false
    name: ""
    portName: http

  configMap:
    create: true
    content: |-
      prometheus.remote_write "mimir" {
        endpoint {
          url     = "http://mimir-distributor.mimir.svc.cluster.local:8080/api/v1/push"
          
          headers = {
            "X-Scope-OrgID" = "anonymous",
          }
        }
      }

      loki.write "loki" {
        endpoint {
          url     = "http://loki-distributor.loki.svc.cluster.local:3100/loki/api/v1/push"
          
          tenant_id = "anonymous"
        }
      }

      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo-distributor.tempo.svc.cluster.local:4318"
          tls {
            insecure = true
          }
        }
      }

      otelcol.processor.batch "default" {
        output {
          metrics = [otelcol.exporter.otlp.tempo.input]
          logs    = [otelcol.exporter.otlp.tempo.input]
          traces  = [otelcol.exporter.otlp.tempo.input]
        }
      }

      otelcol.receiver.otlp "default" {
        http {}
        grpc {}
        
        output {
          metrics = [otelcol.processor.batch.default.input]
          logs    = [otelcol.processor.batch.default.input]
          traces  = [otelcol.processor.batch.default.input]
        }
      }

      tracing {
        sampling_fraction = 1.0
        write_to          = [otelcol.exporter.otlp.tempo.input]
      }

      prometheus.exporter.self "self_metrics" {}

      prometheus.scrape "self_scrape" {
        targets    = prometheus.exporter.self.self_metrics.targets
        forward_to = [prometheus.remote_write.mimir.receiver]
      }

      prometheus.operator.servicemonitors "mimir_smon" {
        forward_to = [prometheus.remote_write.mimir.receiver]

        namespaces = ["mimir"]
      }

      prometheus.operator.servicemonitors "grafana_smon" {
        forward_to = [prometheus.remote_write.mimir.receiver]

        namespaces = ["grafana"]
      }

      mimir.rules.kubernetes "mimir_rules" {
        address = "http://mimir-ruler.mimir.svc.cluster.local:8080"

        tenant_id = "anonymous"

        rule_selector {
          match_labels = {
            "app.kubernetes.io/instance" = "mimir",
          }
        }
      }

      loki.relabel "k3s_nodes_file_logs" {
        forward_to = [loki.write.loki.receiver]

        rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/journal/.*"
          action        = "drop"
        }

        rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/syslog/.*"
          action        = "drop"
        }

         rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/pods/.*"
          action        = "drop"
        }

         rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/containers/.*"
          action        = "drop"
        }

        rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/([^/]+)/.*\\.log"
          target_label  = "filename_parent"
          replacement   = "$1"
        }

        rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/.*/([^/]+)\\.log"
          target_label  = "filename_name"
          replacement   = "$1"
        }

        rule {
          action        = "replace"
          target_label  = "service"
          replacement   = string.format("%s-var-file-logs", sys.env("KUBE_NODE_NAME"))
        }

        rule {
          action       = "replace"
          target_label = "hostname"
          replacement  = sys.env("KUBE_NODE_NAME")
        }

        rule {
          action        = "replace"
          target_label  = "podname"
          replacement   = constants.hostname
        }
      }

      local.file_match "hostlogs" {
        path_targets = [{ __path__ = "/var/log/**/*.log" }]
        sync_period  = "5s"
      }

      loki.source.file "k3s_file_logs" {
        targets      = local.file_match.hostlogs.targets
        forward_to   = [ loki.relabel.k3s_nodes_file_logs.receiver ]
        tail_from_end = true
      }

      loki.relabel "k3s_nodes_journal" {
        forward_to = []

        rule {
          target_label  = "service"
          replacement   = string.format("%s-journal-logs", sys.env("KUBE_NODE_NAME"))
        }

        rule {
          source_labels = ["__journal__systemd_unit"]
          target_label  = "unit"
        }

        rule {
          source_labels = ["__journal__hostname"]
          target_label  = "hostname"
        }

        rule {
          source_labels = ["__journal__PRIORITY"]
          target_label  = "level"
        }

        rule {
          target_label  = "podname"
          replacement   = constants.hostname
        }
      }

      loki.source.journal "k3s_nodes_journal" {
        path           = "/var/log/journal"
        format_as_json = true

        labels = {
          job       = "k3s-systemd-journal",
          component = "k3s-journal-logs",
        }

        relabel_rules = loki.relabel.k3s_nodes_journal.rules
        forward_to    = [loki.write.loki.receiver]
      }

  storagePath: /tmp/alloy

  listenAddr: 0.0.0.0
  listenPort: 12345
  listenScheme: HTTP

  # Telemetry configuration
  enableReporting: true

  mounts:
    varlog: true
    dockercontainers: true
    extra:
      - name: host-journal-run
        mountPath: /run/log/journal
        readOnly: true
      - name: host-journal-var
        mountPath: /var/log/journal
        readOnly: true

  resources: {}

configReloader:
  enabled: true

  resources:
    requests:
      cpu: "10m"
      memory: "50Mi"

controller:
  type: 'daemonset'
  replicas: 1

  extraAnnotations: {}
  podAnnotations: {}
  podLabels: {}

  volumes:
    extra:
      - name: host-journal-run
        hostPath:
          path: /run/log/journal
          type: Directory
      - name: host-journal-var
        hostPath:
          path: /var/log/journal
          type: DirectoryOrCreate

  # -- PodDisruptionBudget configuration.
  podDisruptionBudget:
    # -- Whether to create a PodDisruptionBudget for the controller.
    enabled: false
    # -- Minimum number of pods that must be available during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    minAvailable: null
    # -- Maximum number of pods that can be unavailable during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    maxUnavailable: null

  # -- Whether to enable automatic deletion of stale PVCs due to a scale down operation, when controller.type is 'statefulset'.
  enableStatefulSetAutoDeletePVC: false

  autoscaling:
    horizontal:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80

      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300

      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
    vertical:
      enabled: false
      recommenders: []
      resourcePolicy:
        containerPolicies:
        - containerName: alloy
          controlledResources:
          - cpu
          - memory
          controlledValues: "RequestsAndLimits"
          maxAllowed: {}
          minAllowed: {}

service:
  enabled: true
  type: ClusterIP
  nodePort: 31128
  clusterIP: ''
  internalTrafficPolicy: Cluster
  annotations: {}

serviceMonitor:
  enabled: false
  additionalLabels: {}
  interval: ""
  metricRelabelings: []
  tlsConfig: {}
  relabelings: []

ingress:
  enabled: false
  annotations: {}
  labels: {}
  path: /
  faroPort: 12347
  pathType: Prefix
  hosts:
    - chart-example.local
  extraPaths: []

  tls: []