
alloy:
  clustering:
    enabled: false
    name: ""
    portName: http

  configMap:
    create: true
    content: |-
      prometheus.remote_write "mimir" {
        endpoint {
          url     = "http://mimir-distributor.mimir.svc.cluster.local:8080/api/v1/push"
          
          headers = {
            "X-Scope-OrgID" = "anonymous",
          }
        }
      }

      loki.write "loki" {
        endpoint {
          url     = "http://loki-distributor.loki.svc.cluster.local:3100/loki/api/v1/push"
          
          tenant_id = "anonymous"
        }
      }

      prometheus.exporter.self "self_metrics" {}

      prometheus.scrape "self_scrape" {
        targets    = prometheus.exporter.self.self_metrics.targets
        forward_to = [prometheus.remote_write.mimir.receiver]
      }

      prometheus.operator.servicemonitors "mimir_smon" {
        forward_to = [prometheus.remote_write.mimir.receiver]

        namespaces = ["mimir"]
      }

      prometheus.operator.servicemonitors "grafana_smon" {
        forward_to = [prometheus.remote_write.mimir.receiver]

        namespaces = ["grafana"]
      }

      mimir.rules.kubernetes "mimir_rules" {
        address = "http://mimir-ruler.mimir.svc.cluster.local:8080"

        tenant_id = "anonymous"

        rule_selector {
          match_labels = {
            "app.kubernetes.io/instance" = "mimir",
          }
        }
      }

      loki.relabel "k3s_nodes_journal" {
        forward_to = []

        rule {
          source_labels = ["__journal__systemd_unit"]
          target_label  = "unit"
        }

        rule {
          source_labels = ["__journal__hostname"]
          target_label  = "hostname"
        }

        rule {
          source_labels = ["__journal__PRIORITY"]
          target_label  = "level"
        }
      }

      loki.source.journal "journal" {
        path           = "/var/log/journal"
        format_as_json = true

        labels = {
          job       = "k3s-systemd-journal",
          component = "k3s-journal-logs",
        }

        relabel_rules = loki.relabel.k3s_nodes_journal.rules
        forward_to    = [loki.write.loki.receiver]
      }

      loki.relabel "k3s_nodes_file_logs" {
        rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/([^/]+)/.*\\.log"
          target_label  = "service"
          replacement   = "$1"
        }

        rule {
          source_labels = ["filename"]
          regex         = ".*/var/log/.*/([^/]+)\\.log"
          target_label  = "log_file"
          replacement   = "$1"
        }
      }

      loki.source.file "k3s_file_logs" {
        include = [ "/var/log/**/*.log"]

        exclude = [
          "/var/log/**/*.gz",
          "/var/log/syslog",
          "/var/log/journal",
        ]

        labels = { job = "k3s-file-logs" }

        relabel_rules = loki.relabel.k3s_nodes_file_logs.rules

        forward_to = [loki.write.loki.receiver]
      }



      

  storagePath: /tmp/alloy

  listenAddr: 0.0.0.0
  listenPort: 12345
  listenScheme: HTTP

  # Telemetry configuration
  enableReporting: true

  mounts:
    varlog: true
    dockercontainers: true
    extra:
      - name: host-journal-run
        mountPath: /run/log/journal
        readOnly: true
      - name: host-journal-var
        mountPath: /var/log/journal
        readOnly: true

  resources: {}

configReloader:
  enabled: true

  resources:
    requests:
      cpu: "10m"
      memory: "50Mi"

controller:
  type: 'daemonset'
  replicas: 1

  extraAnnotations: {}
  podAnnotations: {}
  podLabels: {}

  volumes:
    extra:
      - name: host-journal-run
        hostPath:
          path: /run/log/journal
          type: Directory
      - name: host-journal-var
        hostPath:
          path: /var/log/journal
          type: DirectoryOrCreate

  # -- PodDisruptionBudget configuration.
  podDisruptionBudget:
    # -- Whether to create a PodDisruptionBudget for the controller.
    enabled: false
    # -- Minimum number of pods that must be available during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    minAvailable: null
    # -- Maximum number of pods that can be unavailable during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    maxUnavailable: null

  # -- Whether to enable automatic deletion of stale PVCs due to a scale down operation, when controller.type is 'statefulset'.
  enableStatefulSetAutoDeletePVC: false

  autoscaling:
    horizontal:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80

      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300

      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
    vertical:
      enabled: false
      recommenders: []
      resourcePolicy:
        containerPolicies:
        - containerName: alloy
          controlledResources:
          - cpu
          - memory
          controlledValues: "RequestsAndLimits"
          maxAllowed: {}
          minAllowed: {}

service:
  enabled: true
  type: ClusterIP
  nodePort: 31128
  clusterIP: ''
  internalTrafficPolicy: Cluster
  annotations: {}

serviceMonitor:
  enabled: false
  additionalLabels: {}
  interval: ""
  metricRelabelings: []
  tlsConfig: {}
  relabelings: []

ingress:
  enabled: false
  annotations: {}
  labels: {}
  path: /
  faroPort: 12347
  pathType: Prefix
  hosts:
    - chart-example.local
  extraPaths: []

  tls: []